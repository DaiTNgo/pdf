{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import chromadb\n",
    "import openai\n",
    "from PyPDF2 import PdfReader\n",
    "from llama_index import ServiceContext, set_global_service_context, VectorStoreIndex, Document\n",
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index.chat_engine.types import ChatMode\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.indices.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.prompts import PromptTemplate\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"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\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "text_qa_template_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Using both the context information and also using your own knowledge, \"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    "    \"If the context isn't helpful, you can also answer the question on your own. You can choose answer the question by Vietnamese or English\\n\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "# Refine template\n",
    "refine_template_str = (\n",
    "    \"The original question is as follows: {query_str}\\n\"\n",
    "    \"We have provided an existing answer: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing answer \"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Using both the new context and your own knowledge, update or repeat the existing answer.\\n\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)\n",
    "\n",
    "\n",
    "def get_pdf_text(pdf):\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(pdf.file)\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_documents(pdf_docs):\n",
    "    documents = []\n",
    "\n",
    "    for pdf in pdf_docs:\n",
    "        document = Document(\n",
    "            text=get_pdf_text(pdf),\n",
    "            metadata={\n",
    "                \"file_name\": pdf.filename,\n",
    "            },\n",
    "            excluded_llm_metadata_keys=[\"file_name\"],\n",
    "            metadata_seperator=\"::\",\n",
    "            metadata_template=\"{key}=>{value}\",\n",
    "            text_template=\"Metadata: {metadata_str}\\n-----\\nContent: {content}\",\n",
    "        )\n",
    "        documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def get_node_parser():\n",
    "    return SimpleNodeParser.from_defaults()\n",
    "\n",
    "\n",
    "def get_embed_model():\n",
    "    embed_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "    return HuggingFaceEmbedding(model_name=embed_model_name, cache_folder='s_bert')\n",
    "\n",
    "\n",
    "def get_llm_model():\n",
    "    return OpenAI(model='gpt-3.5-turbo', max_tokens=512, temperature=0.1)\n",
    "\n",
    "\n",
    "def get_service_context():\n",
    "    service_context = ServiceContext.from_defaults(\n",
    "        embed_model=get_embed_model(),\n",
    "        node_parser=get_node_parser(),\n",
    "        llm=get_llm_model(),\n",
    "    )\n",
    "    set_global_service_context(service_context)\n",
    "    return service_context\n",
    "\n",
    "\n",
    "def get_vector_store():\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./dbb\")\n",
    "    chroma_collection = chroma_client.get_or_create_collection(name=\"qa-pdf\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def get_storage_context():\n",
    "    storage_context = StorageContext.from_defaults(vector_store=get_vector_store())\n",
    "    return storage_context\n",
    "\n",
    "\n",
    "def rerank():\n",
    "    postprocessor_model_name = \"sentence-transformers/msmarco-distilbert-base-dot-prod-v3\"\n",
    "    postprocessor = SentenceTransformerRerank(\n",
    "        model=postprocessor_model_name,\n",
    "        top_n=3,\n",
    "    )\n",
    "    return postprocessor\n",
    "\n",
    "\n",
    "def get_index():\n",
    "    index = VectorStoreIndex.from_vector_store(\n",
    "        storage_context=get_storage_context(),\n",
    "        vector_store=get_vector_store()\n",
    "    )\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def save_index(documents):\n",
    "    return VectorStoreIndex.from_documents(\n",
    "        documents=documents,\n",
    "        storage_context=get_storage_context(),\n",
    "        vector_store=get_vector_store(),\n",
    "        service_context=get_service_context(),\n",
    "        text_qa_template=text_qa_template,\n",
    "        refine_template=refine_template,\n",
    "        postprocessor=rerank(),\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_answer(instruction):\n",
    "    index = get_index()\n",
    "\n",
    "    chat_engine = index.as_chat_engine(\n",
    "        chat_mode=ChatMode.CONDENSE_QUESTION,\n",
    "        verbose=True,\n",
    "    )\n",
    "    streaming_response = chat_engine.stream_chat(instruction)\n",
    "    return streaming_response\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T01:49:09.068878Z",
     "start_time": "2023-12-22T01:49:07.586390400Z"
    }
   },
   "id": "c2247dbbd6849f22"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/msmarco-distilbert-base-dot-prod-v3 and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Parsing nodes:   0%|          | 0/35 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4208b244fa94ce09d7e6df92301cfee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating embeddings:   0%|          | 0/35 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5caca7c0ae64d9baca85a3188f40b7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<llama_index.indices.vector_store.base.VectorStoreIndex at 0x504e46d0>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_index(SimpleDirectoryReader(\"./benchmarks_data_3\").load_data())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T01:50:03.868169600Z",
     "start_time": "2023-12-22T01:49:09.059213100Z"
    }
   },
   "id": "cc030172e50b3300"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "eval_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"what is natural language processing?\",\n",
    "    \"what is deep learning?\",\n",
    "    \"How businesses are using machine learning?\",\n",
    "    \"What is Retrieval Augmented Generation?\",\n",
    "    \"Who was the first person to come up with the idea for RAG?\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T01:50:03.877123800Z",
     "start_time": "2023-12-22T01:50:03.865523700Z"
    }
   },
   "id": "2fcd9a566e787e39"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "eval_answers = [\n",
    "  \"Machine learning is a field of study that focuses on developing algorithms and models that enable computers to learn and make predictions or decisions without being explicitly programmed. It involves the use of statistical techniques to enable computers to learn from and analyze large amounts of data, identify patterns, and make predictions or decisions based on that data. Machine learning is often used in various applications such as image recognition, natural language processing, and recommendation systems.\",\n",
    "  \"Natural language processing is a field of machine learning where machines learn to understand and interpret human language as it is spoken and written. Instead of relying on traditional programming methods that use data and numbers, natural language processing allows machines to recognize, understand, and respond to language. It also enables machines to generate new text and translate between different languages. Natural language processing is the technology behind familiar applications like chatbots and digital assistants such as Siri or Alexa.\",\n",
    "  'Deep learning refers to a type of machine learning that involves the use of neural networks with multiple layers. These neural networks are comprised of interconnected nodes, or artificial neurons, that have associated weights and thresholds. If the output of a node exceeds the specified threshold, it is activated and passes data to the next layer of the network. Deep learning algorithms typically have more than three layers, including an input and output layer, which is why they are considered \"deep.\" Deep learning has been instrumental in advancing areas such as computer vision, natural language processing, and speech recognition. For a more detailed understanding of the differences between AI, machine learning, deep learning, and neural networks, you can refer to the blog post \"AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What\\'s the Difference?\"',\n",
    "  \"Businesses are using machine learning in various ways. For example, they are using neural networks for natural language translation, image recognition, speech recognition, and image creation. Linear regression is being used to predict numerical values, such as house prices based on historical data. Logistic regression is being used for applications like classifying spam and quality control on a production line. Clustering algorithms are helping businesses identify patterns in data that humans may have overlooked. Decision trees are being used for predicting numerical values and classifying data into categories. Random forests are being used to predict values or categories by combining the results from multiple decision trees.\",\n",
    "  \"Retrieval-Augmented Generation (RAG) is a technique used in generative artificial intelligence (AI) that involves creating text responses based on large language models (LLMs). The AI is trained on a massive amount of data points, which allows it to generate text that is often easy to read and provides detailed responses. RAG works by using information from the training data to generate the response. However, one limitation of RAG is that the information used to generate the response is limited to the information used to train the AI, which can be weeks, months, or even years out of date. In the context of a corporate AI chatbot, this means that the AI may not have specific information about the organization's products or services, which can lead to incorrect responses.\",\n",
    "  \"Sebastian Riedel was the first person to come up with the idea for RAG.\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T01:52:25.172063900Z",
     "start_time": "2023-12-22T01:52:25.140857900Z"
    }
   },
   "id": "e0be921029a0741f"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_eval_questions(q):\n",
    "    res = process_answer(q)\n",
    "    text = ''\n",
    "    for token in res.response_gen:\n",
    "        text += token\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T01:52:25.780598200Z",
     "start_time": "2023-12-22T01:52:25.774072700Z"
    }
   },
   "id": "4b2112d99b8f9693"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# import time\n",
    "# \n",
    "# for q in eval_questions:\n",
    "#     eval_answers.append(get_eval_questions(q))\n",
    "#     time.sleep(22)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T01:52:28.339773900Z",
     "start_time": "2023-12-22T01:52:28.318977Z"
    }
   },
   "id": "23057280db962ec1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Case 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57c4e57b88000e1e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleDirectoryReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 10\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mragas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      3\u001B[0m     faithfulness,\n\u001B[0;32m      4\u001B[0m     answer_relevancy,\n\u001B[0;32m      5\u001B[0m     context_precision,\n\u001B[0;32m      6\u001B[0m     context_recall,\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mragas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllama_index\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m evaluate\n\u001B[1;32m---> 10\u001B[0m documents \u001B[38;5;241m=\u001B[39m \u001B[43mSimpleDirectoryReader\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbenchmarks_data_3\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mload_data()\n\u001B[0;32m     11\u001B[0m vector_index \u001B[38;5;241m=\u001B[39m VectorStoreIndex\u001B[38;5;241m.\u001B[39mfrom_documents(\n\u001B[0;32m     12\u001B[0m     documents, service_context\u001B[38;5;241m=\u001B[39mServiceContext\u001B[38;5;241m.\u001B[39mfrom_defaults(chunk_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m)\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m query_engine \u001B[38;5;241m=\u001B[39m vector_index\u001B[38;5;241m.\u001B[39mas_query_engine()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'SimpleDirectoryReader' is not defined"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    ")\n",
    "from ragas.llama_index import evaluate\n",
    "\n",
    "documents = SimpleDirectoryReader(\"benchmarks_data_3\").load_data()\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=ServiceContext.from_defaults(chunk_size=512)\n",
    ")\n",
    "query_engine = vector_index.as_query_engine()\n",
    "nest_asyncio.apply()\n",
    "eval_answers = [[a] for a in eval_answers]\n",
    "\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "# harmfulness,\n",
    "# result = evaluate(query_engine, metrics, eval_questions, eval_answers)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T01:52:36.166672800Z",
     "start_time": "2023-12-22T01:52:29.667083100Z"
    }
   },
   "id": "1bb72b70bf0d76b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "result = []\n",
    "# time.sleep(60)\n",
    "# result = evaluate(query_engine, metrics, eval_questions, eval_answers)\n",
    "for i in range(len(eval_answers)):\n",
    "    rs = evaluate(query_engine, metrics, eval_questions[i], eval_answers[i])\n",
    "    result.append(rs)\n",
    "    time.sleep(60)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-22T01:52:36.169121400Z"
    }
   },
   "id": "b55b5c13b7b76724"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-22T01:50:55.944920800Z"
    }
   },
   "id": "88a1667f33de1b4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b15fb8b1970f9948"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b6e4a94ec5079d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5620f86fdaeb463a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "72c3c1a946c8e011"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
